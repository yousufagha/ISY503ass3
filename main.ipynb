{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcvCwdpo7ujc",
        "outputId": "074abafa-ddb3-4899-a5db-e0f4459058fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-12-03 15:40:11--  https://www.cs.jhu.edu/~mdredze/datasets/sentiment/domain_sentiment_data.tar.gz\n",
            "Resolving www.cs.jhu.edu (www.cs.jhu.edu)... 128.220.13.64\n",
            "Connecting to www.cs.jhu.edu (www.cs.jhu.edu)|128.220.13.64|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 30586147 (29M) [application/x-gzip]\n",
            "Saving to: ‘domain_sentiment_data.tar.gz.2’\n",
            "\n",
            "domain_sentiment_da 100%[===================>]  29.17M  20.4MB/s    in 1.4s    \n",
            "\n",
            "2024-12-03 15:40:13 (20.4 MB/s) - ‘domain_sentiment_data.tar.gz.2’ saved [30586147/30586147]\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# Download sentiment dataset and extract it for use in training and testing\n",
        "!wget \"https://www.cs.jhu.edu/~mdredze/datasets/sentiment/domain_sentiment_data.tar.gz\"\n",
        "!tar -xzf \"/content/domain_sentiment_data.tar.gz\"\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "from pickle import dump, load\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "import gensim\n",
        "import gensim.downloader as api\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import LSTM, Input, Dense, Embedding, Dropout, BatchNormalization, GRU, Attention\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from flask import Flask, request, render_template\n",
        "\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to clean sentences\n",
        "def clean_sentence(sentence: str) -> list:\n",
        "    \"\"\"\n",
        "    Cleans the given sentence by removing HTML tags, URLs, emails, punctuation, and stopwords.\n",
        "\n",
        "    Args:sentence (str): The sentence to be cleaned.\n",
        "\n",
        "    Returns:list: A list of cleaned words from the input sentence.\n",
        "    \"\"\"\n",
        "    # Remove HTML tags\n",
        "    tags = re.compile(r\"<.*?>\")\n",
        "    sentence = re.sub(tags, '', sentence)\n",
        "    # Convert to lowercase\n",
        "    sentence = sentence.lower()\n",
        "    # Remove URLs and email addresses\n",
        "    email_urls = re.compile(r\"\\bhttp\\S+|\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\b\")\n",
        "    sentence = re.sub(email_urls, '', sentence)\n",
        "    # Replace '@' with 'a'\n",
        "    ats = re.compile(r'@')\n",
        "    sentence = re.sub(ats, 'a', sentence)\n",
        "    # Remove punctuation\n",
        "    punc = re.compile(r\"[^\\w\\s-]\")\n",
        "    sentence = re.sub(punc, '', sentence)\n",
        "    # Tokenize the sentence\n",
        "    sentence = word_tokenize(sentence)\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    sentence = [word for word in sentence if word not in stop_words]\n",
        "    return sentence"
      ],
      "metadata": {
        "id": "HT9GGIn07zX1"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to read training and testing data\n",
        "def read_data(folders, path, regex_review):\n",
        "    \"\"\"\n",
        "    Reads data from the specified folders and extracts sentences for sentiment analysis.\n",
        "\n",
        "    Args:folders (list): List of folder names containing data.\n",
        "        path (str): Base path to the data folders.\n",
        "        regex_review (Pattern): Regular expression pattern to extract review text.\n",
        "\n",
        "    Returns:tuple: A tuple containing lists of data and corresponding labels.\n",
        "    \"\"\"\n",
        "    data, labels = [], []\n",
        "    for folder in folders:\n",
        "        for sentiment, label in [(\"negative\", 0), (\"positive\", 1)]:\n",
        "            file_path = os.path.join(path, folder, f\"{sentiment}.review\")\n",
        "            with open(file_path, 'r') as file:\n",
        "                sentences = re.findall(regex_review, file.read())\n",
        "                cleaned_sentences = [clean_sentence(sentence) for sentence in sentences]\n",
        "                data.extend(cleaned_sentences)\n",
        "                labels.extend([label] * len(cleaned_sentences))\n",
        "    return data, labels\n",
        "\n",
        "# Read training data\n",
        "folders_train = [\"books\", \"dvd\", \"electronics\"]\n",
        "print('Reading Train Data')\n",
        "x_train, y_train = read_data(folders_train, \"/content/sorted_data_acl/\", re.compile(r\"<review_text>.*?</review_text>\", flags=re.DOTALL))\n",
        "\n",
        "# Read testing data\n",
        "folders_test = [\"kitchen_&_housewares\"]\n",
        "print('Reading Test Data')\n",
        "x_test, y_test = read_data(folders_test, \"/content/sorted_data_acl/\", re.compile(r\"<review_text>.*?</review_text>\", flags=re.DOTALL))\n",
        "\n",
        "# Remove short or meaningless reviews (outlier removal)\n",
        "x_train, y_train = zip(*[(x, y) for x, y in zip(x_train, y_train) if len(x) > 3])\n",
        "x_test, y_test = zip(*[(x, y) for x, y in zip(x_test, y_test) if len(x) > 3])\n",
        "\n",
        "# Balance the dataset\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "ros = RandomOverSampler(random_state=42)\n",
        "x_train, y_train = ros.fit_resample(np.array(x_train, dtype=object).reshape(-1, 1), y_train)\n",
        "x_train = x_train.ravel()\n",
        "\n",
        "# Save the preprocessed training and testing data to disk for future use\n",
        "with open('/content/x_train', 'wb') as file:\n",
        "    dump(x_train, file)\n",
        "with open('/content/y_train', 'wb') as file:\n",
        "    dump(y_train, file)\n",
        "with open('/content/x_test', 'wb') as file:\n",
        "    dump(x_test, file)\n",
        "with open('/content/y_test', 'wb') as file:\n",
        "    dump(y_test, file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZFoCXfP7-9k",
        "outputId": "b8d74666-638e-4a1a-d3cf-8c4a5b6e7246"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading Train Data\n",
            "Reading Test Data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a vocabulary from the training data\n",
        "vocab = set(word for sentence in x_train for word in sentence)\n",
        "word2id = {word: idx for idx, word in enumerate(vocab)}\n",
        "id2word = {idx: word for idx, word in enumerate(vocab)}\n",
        "# Assign a dummy value for unknown words\n",
        "dummy = len(word2id)"
      ],
      "metadata": {
        "id": "HfcvhF3R7_dC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode and pad sequences\n",
        "def encode_sentence(old_sentence):\n",
        "    \"\"\"\n",
        "    Encodes a sentence into a list of integers based on the word2id mapping.\n",
        "\n",
        "    Args:old_sentence (list): List of words from a sentence to be encoded.\n",
        "\n",
        "    Returns:list: A list of integers representing the encoded words.\n",
        "    \"\"\"\n",
        "    return [word2id.get(word, dummy) for word in old_sentence]\n",
        "\n",
        "# Encode the training and testing sentences\n",
        "x_train_encoded = [encode_sentence(sentence) for sentence in x_train]\n",
        "x_test_encoded = [encode_sentence(sentence) for sentence in x_test]\n",
        "# Pad the sequences to a fixed length\n",
        "MAX_SEQ_LEN = 125\n",
        "x_train_padded = pad_sequences(x_train_encoded, maxlen=MAX_SEQ_LEN, dtype='int32', padding='post')\n",
        "x_test_padded = pad_sequences(x_test_encoded, maxlen=MAX_SEQ_LEN, dtype='int32', padding='post')\n",
        "\n",
        "# Load GloVe pre-trained embeddings from gensim\n",
        "print('Loading pre-trained word embeddings')\n",
        "glove = api.load('glove-twitter-200')\n",
        "embedding_matrix = np.zeros((len(vocab) + 1, glove.vector_size))\n",
        "for word, idx in word2id.items():\n",
        "    try:\n",
        "        embedding_matrix[idx] = glove[word]\n",
        "    except KeyError:\n",
        "        embedding_matrix[idx] = np.zeros(glove.vector_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcViOaEy8BcN",
        "outputId": "edcfb48c-1374-4f06-ff50-96cb30333468"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pre-trained word embeddings\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the LSTM model for sentiment analysis with optimizations\n",
        "print('Defining LSTM model')\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=len(vocab) + 1, output_dim=glove.vector_size, input_length=MAX_SEQ_LEN, weights=[embedding_matrix], trainable=False))\n",
        "model.add(GRU(128, return_sequences=True))\n",
        "model.add(Dropout(0.4))\n",
        "model.add(BatchNormalization())\n",
        "model.add(GRU(128))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOe4tExv8EE2",
        "outputId": "819e1325-6129-4f64-a4b1-415b441e2a37"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Defining LSTM model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the training data into training and validation sets\n",
        "print('Splitting data into training and validation sets')\n",
        "x_train_padded, x_val_padded, y_train, y_val = train_test_split(x_train_padded, y_train, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fu07JYg88KUq",
        "outputId": "9138baa5-9689-47e1-b28b-e73aee83b176"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Splitting data into training and validation sets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the LSTM model with EarlyStopping to prevent overfitting\n",
        "print('Training the model')\n",
        "y_train = np.array(y_train).reshape(-1, 1)\n",
        "y_val = np.array(y_val).reshape(-1, 1)\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-5)\n",
        "model.fit(x_train_padded, y_train, validation_data=(x_val_padded, y_val), batch_size=16, epochs=100, callbacks=[early_stopping, reduce_lr])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QwYXiwFN8LsU",
        "outputId": "96a86317-5cc8-4976-b268-0c7c77ba7783"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training the model\n",
            "Epoch 1/100\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 293ms/step - accuracy: 0.5147 - loss: 0.7179 - val_accuracy: 0.5008 - val_loss: 0.7057 - learning_rate: 0.0010\n",
            "Epoch 2/100\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 257ms/step - accuracy: 0.5533 - loss: 0.6742 - val_accuracy: 0.7083 - val_loss: 0.5408 - learning_rate: 0.0010\n",
            "Epoch 3/100\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 246ms/step - accuracy: 0.7894 - loss: 0.4795 - val_accuracy: 0.7575 - val_loss: 0.5691 - learning_rate: 0.0010\n",
            "Epoch 4/100\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 266ms/step - accuracy: 0.8225 - loss: 0.4175 - val_accuracy: 0.7567 - val_loss: 0.5314 - learning_rate: 0.0010\n",
            "Epoch 5/100\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 248ms/step - accuracy: 0.8546 - loss: 0.3643 - val_accuracy: 0.7442 - val_loss: 0.5996 - learning_rate: 0.0010\n",
            "Epoch 6/100\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 248ms/step - accuracy: 0.8900 - loss: 0.2832 - val_accuracy: 0.7675 - val_loss: 0.6111 - learning_rate: 0.0010\n",
            "Epoch 7/100\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 246ms/step - accuracy: 0.9087 - loss: 0.2516 - val_accuracy: 0.7483 - val_loss: 0.6441 - learning_rate: 0.0010\n",
            "Epoch 8/100\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 247ms/step - accuracy: 0.9492 - loss: 0.1650 - val_accuracy: 0.7825 - val_loss: 0.7955 - learning_rate: 2.0000e-04\n",
            "Epoch 9/100\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 247ms/step - accuracy: 0.9614 - loss: 0.1332 - val_accuracy: 0.7958 - val_loss: 0.8026 - learning_rate: 2.0000e-04\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x792df7e236d0>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test data\n",
        "y_test = np.array(y_test).reshape(-1, 1)\n",
        "y_test = y_test.astype('float32')\n",
        "loss, accuracy = model.evaluate(x_test_padded, y_test)\n",
        "print(f'Test accuracy: {accuracy * 100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-eQ9ZX808M_s",
        "outputId": "1bb3d3e9-a6e4-4546-97ea-1c188f9a68d8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 128ms/step - accuracy: 0.8437 - loss: 0.3684\n",
            "Test accuracy: 79.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print classification report and confusion matrix\n",
        "print('Classification Report:')\n",
        "y_pred = (model.predict(x_test_padded) > 0.5).astype('int32')\n",
        "print(classification_report(y_test, y_pred, target_names=['Negative', 'Positive']))\n",
        "print('Confusion Matrix:')\n",
        "print(confusion_matrix(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V0tfjbGmFKV4",
        "outputId": "a574ef8d-adb7-408b-bd1e-0f68c40543ab"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 99ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Negative       0.75      0.87      0.81      1000\n",
            "    Positive       0.84      0.71      0.77      1000\n",
            "\n",
            "    accuracy                           0.79      2000\n",
            "   macro avg       0.80      0.79      0.79      2000\n",
            "weighted avg       0.80      0.79      0.79      2000\n",
            "\n",
            "Confusion Matrix:\n",
            "[[867 133]\n",
            " [287 713]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom prediction function\n",
        "def lstm_predict():\n",
        "    \"\"\"\n",
        "    Takes user input, preprocesses it, and uses the trained LSTM model to predict the sentiment.\n",
        "\n",
        "    Returns:\n",
        "        tuple: The sentiment result and confidence score.\n",
        "    \"\"\"\n",
        "    sentence = input(\"Enter a sentence to assess its sentiment: \")\n",
        "    ready_sentence = pad_sequences([encode_sentence(clean_sentence(sentence))], maxlen=MAX_SEQ_LEN, dtype='int32', padding='post')\n",
        "    score = model.predict(ready_sentence)[0][0]\n",
        "    confidence = score if score >= 0.5 else 1 - score\n",
        "    result = \"Positive Review\" if score >= 0.5 else \"Negative Review\"\n",
        "    confidence_str = \"High\" if confidence > 0.75 else (\"Medium\" if confidence > 0.5 else \"Low\")\n",
        "    print(f\"{result} (Score: {score:.2f}, Confidence: {confidence:.2f}, Confidence Level: {confidence_str})\")\n",
        "    return result, confidence\n",
        "\n",
        "# Prompt the user to enter a sentence and predict its sentiment\n",
        "lstm_predict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NgEHlt3n8Oo7",
        "outputId": "5c2e212e-29ce-4d70-8322-d04868efef83"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a sentence to assess its sentiment: it is a bad product\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
            "Negative Review (Score: 0.01, Confidence: 0.99, Confidence Level: High)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Negative Review', 0.9867006251588464)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_predict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZAqSJPOLWJeo",
        "outputId": "65a14f18-d755-4840-f253-af3b25b30f32"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a sentence to assess its sentiment: it is a good product\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
            "Positive Review (Score: 0.91, Confidence: 0.91, Confidence Level: High)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Positive Review', 0.9146273)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Flask web application for sentiment analysis\n",
        "app = Flask(__name__)\n",
        "\n",
        "@app.route('/')\n",
        "def home():\n",
        "    \"\"\"Renders the homepage with a form to input text.\"\"\"\n",
        "    return render_template('index.html')\n",
        "\n",
        "@app.route('/predict', methods=['POST'])\n",
        "def predict():\n",
        "    \"\"\"\n",
        "    Handles the form submission, processes the input text, and returns the sentiment prediction.\n",
        "\n",
        "    Returns:\n",
        "        str: Rendered HTML template with prediction result.\n",
        "    \"\"\"\n",
        "    if request.method == 'POST':\n",
        "        sentence = request.form['sentence']\n",
        "        ready_sentence = pad_sequences([encode_sentence(clean_sentence(sentence))], maxlen=MAX_SEQ_LEN, dtype='int32', padding='post')\n",
        "        score = model.predict(ready_sentence)[0][0]\n",
        "        confidence = score if score >= 0.5 else 1 - score\n",
        "        result = \"Positive Review\" if score >= 0.5 else \"Negative Review\"\n",
        "        return render_template('result.html', result=result, score=f\"{score:.2f}\", confidence=f\"{confidence:.2f}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Run the Flask web server\n",
        "    app.run(debug=True)\n",
        "\n",
        "# Note: To run the web server, Flask templates (HTML files) should be created, such as 'index.html' for the form input and 'result.html' to display the prediction results.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VSVvQ9F8Qo5",
        "outputId": "dfae0288-97d9-4988-d969-b837b816248e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: on\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "INFO:werkzeug: * Restarting with stat\n"
          ]
        }
      ]
    }
  ]
}